{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpxiDT65C0dw",
        "outputId": "178c3a91-ae24-4916-ce7f-9211ec74913c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "BERTimbau ‚Äî pipeline padronizado ao BERTweetBR\n",
        "- Mesma m√°scara de emojis/emoticons (anti-atalho)\n",
        "- Split por leak_key (sem vazamento)\n",
        "- Tokeniza√ß√£o + padding din√¢mico\n",
        "- Class weights + label smoothing na loss\n",
        "- Early stopping (F1), load_best_model_at_end\n",
        "- Salvamento de m√©tricas e matriz de confus√£o\n",
        "\"\"\"\n",
        "\n",
        "!pip -q install \"transformers>=4.41,<4.47\" datasets accelerate \\\n",
        "                scikit-learn emoji unidecode imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback, set_seed\n",
        ")\n",
        "\n",
        "from emoji import replace_emoji, demojize\n",
        "from unidecode import unidecode\n"
      ],
      "metadata": {
        "id": "x5HIQYtaDegy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Drive (opcional)\n",
        "# ----------------------------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception as e:\n",
        "    print(\"Aviso: n√£o foi poss√≠vel montar o Drive automaticamente. Se estiver fora do Colab, ignore.\")\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFSck97VDsVA",
        "outputId": "a31a26b7-b122-4371-da7c-b5addc38e47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH  = \"/content/drive/MyDrive/versaochat/twitter-sentiment-pt-BR-md-2-l.csv\"  # <- ajuste se precisar\n",
        "RUN_TS    = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "BASE_DIR  = \"/content/drive/MyDrive/versaochat\"\n",
        "OUT_DIR   = f\"{BASE_DIR}/bertimbau-finetuned-sentiment-{RUN_TS}\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "TEXT_COL   = \"tweet_text\"\n",
        "LABEL_COL  = \"sentiment\"\n",
        "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"\n",
        "SEED       = 42\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); set_seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "print(\"OUT_DIR:\", OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n06w7JfnD3y5",
        "outputId": "7c5c80f3-b303-4f9b-fa22-bc569fff6494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "OUT_DIR: /content/drive/MyDrive/versaochat/bertimbau-finetuned-sentiment-20250820-161230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Carregamento do CSV\n",
        "# ============================\n",
        "def load_csv(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except UnicodeDecodeError:\n",
        "        return pd.read_csv(path, encoding=\"latin-1\")\n",
        "\n",
        "df = load_csv(CSV_PATH).copy()\n",
        "assert TEXT_COL in df.columns and LABEL_COL in df.columns, f\"Esperado colunas: {TEXT_COL}, {LABEL_COL}\"\n",
        "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).copy()\n",
        "df[TEXT_COL] = df[TEXT_COL].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "\n",
        "# R√≥tulos 0/1 -> cria coluna 'labels' (padr√£o do Trainer)\n",
        "if not pd.api.types.is_integer_dtype(df[LABEL_COL]):\n",
        "    raise ValueError(f\"A coluna {LABEL_COL} deve ser int (0/1).\")\n",
        "df[\"labels\"] = df[LABEL_COL].astype(int)\n",
        "\n",
        "num_labels = len(sorted(df[\"labels\"].unique()))\n",
        "assert num_labels in (2,3), f\"num_labels inesperado: {num_labels}\"\n",
        "print(\"Contagem de classes:\", df[\"labels\"].value_counts().to_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMyB6CzBEAyp",
        "outputId": "f7c5701e-9435-48ce-d436-cdec148f73b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contagem de classes: {0: 10001, 1: 9999}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) M√°scara de emojis/emoticons (anti-atalho)\n",
        "# ============================\n",
        "_POS = re.compile(r'(:\\)|:-\\)|=\\)|:\\]|:D|:-D|;\\)|;-\\)|:P|:-P|<3+)', re.I)\n",
        "_NEG = re.compile(r'(:\\(|:-\\(|=\\(|:\\[|:\\'\\(|D:)', re.I)\n",
        "\n",
        "def mask_emotes_and_emojis(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = replace_emoji(s, replace=\" EMOJI \")  # todos emojis -> EMOJI\n",
        "    s = _POS.sub(\" EMOTE \", s)               # emoticons positivos -> EMOTE\n",
        "    s = _NEG.sub(\" EMOTE \", s)               # emoticons negativos -> EMOTE\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "df[\"text_masked\"] = df[TEXT_COL].apply(mask_emotes_and_emojis)\n"
      ],
      "metadata": {
        "id": "pijZOPS_EGpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Split sem vazamento por grupos (leak_key)\n",
        "# ============================\n",
        "_url_re   = re.compile(r\"http\\S+|www\\.\\S+\", re.IGNORECASE)\n",
        "_mention  = re.compile(r\"@\\w+\")\n",
        "_ws       = re.compile(r\"\\s+\")\n",
        "\n",
        "def norm_for_leak_key(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    s = _url_re.sub(\" \", s)\n",
        "    s = _mention.sub(\" \", s)\n",
        "    s = demojize(s, language=\"pt\")  # seguran√ßa extra\n",
        "    s = unidecode(s).lower()\n",
        "    s = _ws.sub(\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "df[\"leak_key\"] = df[\"text_masked\"].apply(norm_for_leak_key)\n",
        "\n",
        "rep = df.drop_duplicates(subset=[\"leak_key\"])[[\"leak_key\",\"labels\"]].copy()\n",
        "rep_temp, rep_test = train_test_split(rep, test_size=0.10, random_state=SEED, stratify=rep[\"labels\"])\n",
        "rep_train, rep_val = train_test_split(rep_temp, test_size=0.1111, random_state=SEED, stratify=rep_temp[\"labels\"])\n",
        "\n",
        "key2split = {k:\"train\" for k in rep_train[\"leak_key\"]}\n",
        "key2split.update({k:\"val\" for k in rep_val[\"leak_key\"]})\n",
        "key2split.update({k:\"test\" for k in rep_test[\"leak_key\"]})\n",
        "df[\"split\"] = df[\"leak_key\"].map(key2split)\n",
        "assert df[\"split\"].isna().sum()==0\n",
        "\n",
        "def _inter(a,b): return len(set(a).intersection(set(b)))\n",
        "lk_tr = df.query(\"split=='train'\")[\"leak_key\"]\n",
        "lk_va = df.query(\"split=='val'\")[\"leak_key\"]\n",
        "lk_te = df.query(\"split=='test'\")[\"leak_key\"]\n",
        "print(\"Leak check (esperado 0,0,0):\", _inter(lk_tr, lk_va), _inter(lk_tr, lk_te), _inter(lk_va, lk_te))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfxIaGllQU1E",
        "outputId": "b3bf2f65-cb75-4317-fb8e-52d383aed03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leak check (esperado 0,0,0): 0 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Dataset HF (texto mascarado)\n",
        "# ============================\n",
        "ds = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(\n",
        "        df[df[\"split\"]==\"train\"][[\"text_masked\",\"labels\"]]\n",
        "          .rename(columns={\"text_masked\":\"text\"})\n",
        "          .reset_index(drop=True)\n",
        "    ),\n",
        "    \"validation\": Dataset.from_pandas(\n",
        "        df[df[\"split\"]==\"val\"][[\"text_masked\",\"labels\"]]\n",
        "          .rename(columns={\"text_masked\":\"text\"})\n",
        "          .reset_index(drop=True)\n",
        "    ),\n",
        "    \"test\": Dataset.from_pandas(\n",
        "        df[df[\"split\"]==\"test\"][[\"text_masked\",\"labels\"]]\n",
        "          .rename(columns={\"text_masked\":\"text\"})\n",
        "          .reset_index(drop=True)\n",
        "    ),\n",
        "})\n"
      ],
      "metadata": {
        "id": "7sO1h-R1QfT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Tokenizador + tokeniza√ß√£o\n",
        "# ============================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)  # sem normalization (n√£o existe para BERTimbau)\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=False, max_length=128)\n",
        "\n",
        "ds_tok = ds.map(tokenize_batch, batched=True, desc=\"Tokenizando (mascarado)\")\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397,
          "referenced_widgets": [
            "23c4cb642071402a9f38221ae2ba65f7",
            "c548544062884dc29c3a3caced41026f",
            "417aa9307fe948e9a218513922ea6fc7",
            "c2db0bc7872f4f4ebaf9a98b51186849",
            "c0741564e7e243a18442999fed8a6015",
            "dd1e58a074274289ae999429c55b0c5a",
            "626a51729c8547dab5fc5eec071d004c",
            "b6fc97e346ab4ebfbee2113ad8b5bcf3",
            "063ba899a45e4e2db9e109a319dca26d",
            "f709971763004eba87755df8d6c044ce",
            "811dbb53f0e048aebb02c87426c291de",
            "d416ea410136429a888f1f05d7e98649",
            "64eee912b01240c4a9ccb077b02bf074",
            "248964d593ba4e60bd0bc5fbe7a3717b",
            "967096d3df4f41dbba7c289a9f958b3c",
            "5872f3c10fef40e1aef06e5be607d41d",
            "3ff85fa0fef74341bc2167c66d9c4b51",
            "7eab829dfae345aea78aaacc2e65e46f",
            "01d1626a95c644afbcff09f7255e8ab1",
            "52b39870b8c8498e88fcaf76f2d9fcbf",
            "a899431325c0471f8c465625cd454e00",
            "002a567c112c4c5499c04e3002de2170",
            "02254b6444da499ba20831af6cd91696",
            "733b6472f2d049a49d17967de39c03b0",
            "a0dba145d20a4407945f38cb8716ebea",
            "cc80d13d0ff4471191157c8e221593f9",
            "2ab13b1f62844a9ea2b26444982f05ae",
            "fc15f939118344c7aba41f00f470fec2",
            "15c945f2de4548ec817fb2b019ad6e8b",
            "7e06884ae69e4c67b8137023c9751f0f",
            "617eaf167ba8401caf4149c3613512dc",
            "56a4659903f9448a8a98727f96f46437",
            "ca195bde5aae4845a12a04f0b3d55a05",
            "5f193a5ea5d649c6b4a55658b261216e",
            "66d7a950075d462db497e71d89e2852f",
            "6834b644ee8f4201ad0d78d58251050a",
            "8e5552d9c4554b00b1017e9725f1c6da",
            "55e8d47fae9c44f1b56daa31334779b6",
            "922da0fef18c495281e41d3b8a3abe79",
            "3db8fae5070244e9b86c8543dee36d0c",
            "d23a0e5f220244f28d901012f98d75f6",
            "9e064948b937441097dce193fc84cf25",
            "5b2630b44bf14af9b2eb5af2a440b38e",
            "510101e064904a77b772d550abb8c729",
            "e19301f0de4a44fd8656f3f5262e8d34",
            "ed35e841c6e8414b85bb2959cf9ff7fb",
            "1c63f7b8f8154057b3f55bce8f0e0c14",
            "28ada3ed6abb418c90751684ae687dfa",
            "17b859210d1d49bda905cd2df83c24f7",
            "20ea621a26884479a06ec47013ae18f9",
            "4a3e2505142a452392cda35a4ded61ad",
            "71e171ef659e460db88cdb782cc48024",
            "b431489c6fa14cd78ea4cce9c9f43208",
            "2f5b0bb151ec4dfcb7898272338c1b0d",
            "b52c582408d54670b0577f469260f5cd",
            "ec5f6d4f74a8456dabfe9ac50d8dd625",
            "c63e13f370be4dc786b2ed5bb093efb6",
            "b4d967c96c76403291b73f23811fa2ad",
            "bd683707df61425aad0ee04f5ee0b881",
            "4cd7f1e07b38414a8d3d34a8d8203040",
            "9e190d5d2cf646c09ac53da19bc4d76c",
            "42db9792830943dfa98d7acf4133153f",
            "3664c1497a0e4bc8bc8cbb61fe755051",
            "b1b826e888cd4fd9bab0e802801db716",
            "70d62f35c14d4a20955560e1c73763fb",
            "1799ada0a8fd4ccd9b56af698bcfe275",
            "4390f88adf4b4c71acfbc20097300486",
            "9344519595e4482c947cb42568dc0f6d",
            "b3aaf574ba2846c5bf80c14fbeab8d83",
            "81e947e79960442eb9a057a5d6dd719a",
            "668a3589a3ad4f54b18cea37d0d6d71b",
            "9d22325b93894e0fa26aaafa19c419dd",
            "9eb3cc543c32422cb4e9097a6a055760",
            "ea9b365ff55e42fd87e3a80c76d6699b",
            "663d171cd8104e44b606f6460a311af1",
            "ac700ee6148741ceb8602571b199b9a6",
            "86fef061435748cc8a85f78d201f821a",
            "5c4a9288db874068a7fcb2461b906a99",
            "cba4c86106b84fe5b2205d8938066559",
            "66dee6ae6d11435e89b22c77829e4183",
            "85e68a7eac36435f8ff4b939dfedde6d",
            "cc4011f8750548a4a12cd0aa69da987a",
            "bd50f318fa554d4ebaf6be12fb47f9b5",
            "fa46768fc98c4ce89fc4a423b7d3df30",
            "d8b27215fc3546928cc7c1aaa6d0711b",
            "0f08bcae329344cf947f99033faf0fc7",
            "cfc1b0644f324e42af68248b2715ed7a",
            "00155b96a1534bc79d0a3cd9289bd5b1"
          ]
        },
        "id": "-pq49tcZQlJT",
        "outputId": "d87c189e-661b-42ac-eda7-3cffcb0c7c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23c4cb642071402a9f38221ae2ba65f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d416ea410136429a888f1f05d7e98649"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02254b6444da499ba20831af6cd91696"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f193a5ea5d649c6b4a55658b261216e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e19301f0de4a44fd8656f3f5262e8d34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizando (mascarado):   0%|          | 0/15980 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec5f6d4f74a8456dabfe9ac50d8dd625"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizando (mascarado):   0%|          | 0/2008 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4390f88adf4b4c71acfbc20097300486"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizando (mascarado):   0%|          | 0/2012 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c4a9288db874068a7fcb2461b906a99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Modelo + Trainer (class weights, early stopping)\n",
        "# ============================\n",
        "config = AutoConfig.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=num_labels,\n",
        "    id2label={i:str(i) for i in range(num_labels)},\n",
        "    label2id={str(i):i for i in range(num_labels)},\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config).to(device)\n",
        "\n",
        "# Pesos de classe\n",
        "y_train = np.array(ds_tok[\"train\"][\"labels\"])\n",
        "classes_sorted = np.unique(y_train)\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes_sorted, y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Trainer com CE + class weights + label smoothing consistente\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits  = outputs.get(\"logits\")\n",
        "        loss = nn.functional.cross_entropy(\n",
        "            logits.view(-1, model.config.num_labels),\n",
        "            labels.view(-1),\n",
        "            weight=self.class_weights,\n",
        "            label_smoothing=0.05  # mantenha igual ao BERTweetBR; ou 0.0 em ambos\n",
        "        )\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    evaluation_strategy=\"epoch\",      # <- CORRIGIDO (antes: eval_strategy)\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",  # <- CORRIGIDO (antes: \"f1\")\n",
        "    greater_is_better=True,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=6,\n",
        "    warmup_ratio=0.06,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    gradient_accumulation_steps=1,\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",\n",
        "    seed=SEED,\n",
        "    save_total_limit=2               # opcional, evita muitos checkpoints\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": p,\n",
        "        \"recall\": r,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_tok[\"train\"],\n",
        "    eval_dataset=ds_tok[\"validation\"],\n",
        "    tokenizer=tokenizer,              # <- OK\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(\n",
        "        early_stopping_patience=2,\n",
        "        early_stopping_threshold=0.0  # opcional, expl√≠cito\n",
        "    )],\n",
        "    class_weights=class_weights,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAH3Z2JKQmW7",
        "outputId": "6c0d0831-f10b-44dc-9999-7d72c74cce71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-976248030.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Treino\n",
        "# ============================\n",
        "train_result = trainer.train()\n",
        "print(\"Melhor checkpoint:\", trainer.state.best_model_checkpoint)\n",
        "\n",
        "# ============================\n",
        "# 9) Avalia√ß√£o + Relat√≥rios + Salvamento\n",
        "# ============================\n",
        "print(\"\\n== VAL (mascarado) ==\")\n",
        "val_metrics = trainer.evaluate(ds_tok[\"validation\"])\n",
        "val_metrics = {k: float(v) for k, v in val_metrics.items()}\n",
        "print({k: round(v, 4) for k, v in val_metrics.items()})\n",
        "\n",
        "print(\"\\n== TEST (mascarado) ==\")\n",
        "test_metrics = trainer.evaluate(ds_tok[\"test\"])\n",
        "test_metrics = {k: float(v) for k, v in test_metrics.items()}\n",
        "print({k: round(v, 4) for k, v in test_metrics.items()})\n",
        "\n",
        "pred_test = trainer.predict(ds_tok[\"test\"])\n",
        "y_true = pred_test.label_ids\n",
        "y_pred = np.argmax(pred_test.predictions, axis=-1)\n",
        "\n",
        "print(\"\\nClassification report (TEST):\")\n",
        "print(classification_report(y_true, y_pred, target_names=[str(i) for i in range(num_labels)], zero_division=0))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "\n",
        "# salva m√©tricas em JSON e CM em CSV\n",
        "with open(os.path.join(OUT_DIR, \"metrics_val.json\"), \"w\") as f:\n",
        "    json.dump(val_metrics, f, indent=2)\n",
        "with open(os.path.join(OUT_DIR, \"metrics_test.json\"), \"w\") as f:\n",
        "    json.dump(test_metrics, f, indent=2)\n",
        "\n",
        "pd.DataFrame(cm, columns=[f\"pred_{i}\" for i in range(num_labels)], index=[f\"true_{i}\" for i in range(num_labels)])\\\n",
        "  .to_csv(os.path.join(OUT_DIR, \"confusion_matrix_test.csv\"), index=True)\n",
        "\n",
        "# salva modelo + tokenizer\n",
        "from pathlib import Path\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "tokenizer.save_pretrained(OUT_DIR)\n",
        "model.save_pretrained(OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "EllufnQHQudb",
        "outputId": "78a1981f-92a2-4a05-e734-5ffc4cf8dd37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3996' max='5994' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3996/5994 07:54 < 03:57, 8.42 it/s, Epoch 4/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.420700</td>\n",
              "      <td>0.409090</td>\n",
              "      <td>0.829681</td>\n",
              "      <td>0.829765</td>\n",
              "      <td>0.829681</td>\n",
              "      <td>0.829641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.349500</td>\n",
              "      <td>0.394483</td>\n",
              "      <td>0.849602</td>\n",
              "      <td>0.849724</td>\n",
              "      <td>0.849602</td>\n",
              "      <td>0.849561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.260400</td>\n",
              "      <td>0.513095</td>\n",
              "      <td>0.836155</td>\n",
              "      <td>0.836280</td>\n",
              "      <td>0.836155</td>\n",
              "      <td>0.836108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.187200</td>\n",
              "      <td>0.549370</td>\n",
              "      <td>0.837151</td>\n",
              "      <td>0.837334</td>\n",
              "      <td>0.837151</td>\n",
              "      <td>0.837091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor checkpoint: /content/drive/MyDrive/versaochat/bertimbau-finetuned-sentiment-20250820-161230/checkpoint-1998\n",
            "\n",
            "== VAL (mascarado) ==\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.3945, 'eval_accuracy': 0.8496, 'eval_precision': 0.8497, 'eval_recall': 0.8496, 'eval_f1': 0.8496, 'eval_runtime': 2.5179, 'eval_samples_per_second': 797.496, 'eval_steps_per_second': 25.021, 'epoch': 4.0}\n",
            "\n",
            "== TEST (mascarado) ==\n",
            "{'eval_loss': 0.3894, 'eval_accuracy': 0.8439, 'eval_precision': 0.8454, 'eval_recall': 0.8439, 'eval_f1': 0.8438, 'eval_runtime': 2.7134, 'eval_samples_per_second': 741.51, 'eval_steps_per_second': 23.218, 'epoch': 4.0}\n",
            "\n",
            "Classification report (TEST):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       995\n",
            "           1       0.87      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2012\n",
            "   macro avg       0.85      0.84      0.84      2012\n",
            "weighted avg       0.85      0.84      0.84      2012\n",
            "\n",
            "Confusion matrix:\n",
            " [[870 125]\n",
            " [189 828]]\n"
          ]
        }
      ]
    }
  ]
}